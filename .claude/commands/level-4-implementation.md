## Usage

```
/level-4-implementation <feature-artifact-file>
```

**Input**: Level 3 feature YAML artifact (e.g., `level-3-similarity-authentication.yml`)
**Output**: Implementation task YAML artifact for the feature (e.g., `level-4-similarity-authentication-implementation.yml`)

## Command Guidelines

### Context File Format Reminder

**Context File**:
  Structured document preserving critical project decisions across development phases.

**IMPORTANT**:
  Context files generated by the SAID command set are stored in YAML format (`.yml` extension), not Markdown.
  They are located in the `context/` directory.

- Follow these guidelines when generating context files.
- Context files need not be the only artifacts generated.
- Report files are in Markdown (`.md` extension). They are located in the `docs/reports/` directory.

### Working Implementation Expert

**Focus**: Transform Level 3 specifications into production-ready, tested, deployable code
**Success criteria**: Complete working system that meets Level 0 vision, passes all tests, ready for production
**Failure mode alert**: Cutting corners on testing/security or optimizing prematurely instead of delivering working system
**Transition requirement**: Full system working end-to-end with comprehensive test coverage
**Output**: Production-ready code + implementation task YAML artifact + deployment documentation

### AI Collaboration Framework Reminder

1. **Question Before Acting** - Always ask clarifying questions before major implementation decisions
2. **Present Options** - Provide pros/cons rather than choosing for the user
3. **Explain Reasoning** - Share the "why" behind technical recommendations
4. **Pause for Input** - Honor collaboration checkpoints at key decision points
5. **Calibrate Confidence** - AI involvement scales inversely with domain complexity

### Level 4 Specific Guidelines

**Working Implementation Protocol**

**1. Implementation Planning**
- Break specifications into implementable tasks (1-2 day chunks)
- Prioritize critical path and dependencies
- Plan integration points and testing approach
- Establish code quality and review standards
- **COLLABORATION CHECKPOINT**: Present implementation tasks and discuss priorities with user

**2. Code Implementation**
- Follow established coding standards and patterns
- Implement comprehensive error handling
- Add proper logging and monitoring
- Write self-documenting code with clear interfaces
- **COLLABORATION CHECKPOINT**: Present coding approaches and patterns, get user input

**3. Testing Strategy**
- Unit tests for all business logic
- Integration tests for component interactions
- End-to-end tests for critical user workflows
- Performance tests for key scenarios
- **COLLABORATION CHECKPOINT**: Present testing strategy, confirm coverage expectations with user

**4. Security Implementation**
- Input validation and sanitization
- Authentication and authorization
- Data protection and encryption
- Security logging and monitoring
- **COLLABORATION CHECKPOINT**: Present security approach, validate requirements with user

**5. Operational Readiness**
- Deployment scripts and configuration
- Monitoring and alerting setup
- Documentation for operations team
- Rollback and recovery procedures
- **COLLABORATION CHECKPOINT**: Present deployment strategy, discuss operational concerns with user

### Feature-to-Implementation Decomposition and Task Artifact Generation

**Feature-to-Implementation Decomposition**:
Level 4 processes ONE feature at a time, breaking it down into specific implementation tasks.

**Input Processing**:
1. **Feature Artifact Analysis**: Parse Level 3 feature YAML artifact
2. **Feature Understanding**: Extract feature specifications, requirements, and constraints
3. **Task Identification**: Break feature into implementable tasks (1-2 day chunks)
4. **Task Validation**: Ensure tasks are realistic and achievable by team

**Implementation Task YAML Artifact (Required)**:
For each feature, create a structured implementation task YAML artifact:

```yaml
# Use template: context/templates/level-4-implementation-artifact.yml
# Generate one artifact per feature: level-4-{component-name}-{feature-name}-implementation.yml

artifact_requirements:
  - One YAML file per feature implementation
  - Use consistent naming: level-4-{component-name}-{feature-name}-implementation.yml
  - Contain all implementation tasks, timelines, and resource requirements
  - Include traceability to parent feature and component
  - Document risk management and quality assurance plans
  - Specify final deliverables and post-implementation requirements
```

**Implementation Decomposition Process**:
1. **Feature Artifact Analysis**: Parse and analyze Level 3 feature YAML artifact
2. **Implementation Planning**: Break feature into implementable tasks
3. **TASK BREAKDOWN VALIDATION**: Present implementation tasks and discuss priorities with user
4. **Task Specification**: Create detailed task specifications with effort estimates
5. **ARTIFACT GENERATION**: Generate implementation task YAML artifact
6. **ARTIFACT REVIEW**: Review generated artifact with user for completeness
7. **Implementation Execution**: Execute tasks according to plan
8. **Progress Tracking**: Update artifact with progress and lessons learned

**Task Categories**:
- **Development Tasks**: Core implementation, business logic, APIs
- **Testing Tasks**: Unit tests, integration tests, E2E tests, performance tests
- **Infrastructure Tasks**: Deployment, monitoring, security, configuration
- **Documentation Tasks**: API docs, user guides, technical documentation

**Task Sizing Guidelines**:
- **Small Tasks**: 2-4 hours (single function, component method)
- **Medium Tasks**: 1 day (complete feature, component integration)
- **Large Tasks**: 2 days maximum (major component, complex integration)

**Implementation Tracking**:
- Progress tracking with completion percentages
- Issue tracking with severity and resolution approaches
- Change tracking with impact assessment
- Lessons learned for future improvements

**Quality Assurance Integration**:
- Code quality standards and review requirements
- Testing requirements and coverage goals
- Quality gates and acceptance criteria
- Security and operational requirements

### Implementation Order Priority

**Priority Guidelines**:
1. Core business logic (validates Level 0 vision)
2. Critical integration points (proves Level 2 structure)
3. User-facing features (demonstrates user value)
4. Edge case handling (robustness)
5. Performance optimization (only if needed)

### Quality Standards

**Code Quality Requirements**:
```yaml
quality_standards:
  code_structure:
    functions: "Single responsibility, max 20 lines"
    classes: "Clear purpose, minimal dependencies"
    modules: "Cohesive functionality, stable interfaces"

  error_handling:
    validation: "All inputs validated"
    exceptions: "Proper exception types and messages"
    recovery: "Graceful degradation where possible"

  testing:
    unit_coverage: "80% for business logic"
    integration_coverage: "100% for critical paths"
    e2e_coverage: "All major user workflows"

  documentation:
    api_docs: "Complete with examples"
    readme: "Setup and deployment instructions"
    decisions: "Non-obvious choices explained"
```

### Testing Implementation

**Test Categories and Implementation**:

```typescript
// Unit Test Example Structure
describe('ComponentName', () => {
  describe('businessLogicMethod', () => {
    it('should handle normal case correctly', () => {
      // Test normal operation
    });

    it('should handle edge cases gracefully', () => {
      // Test boundary conditions
    });

    it('should throw appropriate errors for invalid input', () => {
      // Test error conditions
    });
  });
});

// Integration Test Structure
describe('Component Integration', () => {
  it('should handle end-to-end workflow', async () => {
    // Test complete workflow across components
  });

  it('should handle external service failures', async () => {
    // Test resilience to external failures
  });
});
```

**Performance Testing**:
```yaml
performance_tests:
  load_testing:
    target: "Expected concurrent users"
    duration: "Sustained load test duration"
    acceptance_criteria: "Response time and error rate thresholds"

  stress_testing:
    target: "2x expected load"
    purpose: "Find breaking point"
    acceptance_criteria: "Graceful degradation"
```

### Context Manifest Completion

Final context manifest with implementation results:

```yaml
level_4_context:
  # Inherit from Level 3
  implementation_results:
    components_completed:
      - component: "Component name"
        status: "COMPLETED/IN_PROGRESS/BLOCKED"
        test_coverage: "Percentage coverage"
        performance_results: "Actual vs. target performance"
        known_issues: ["List of known limitations"]

    integration_results:
      - integration: "Integration description"
        status: "WORKING/ISSUES/FAILED"
        test_results: "Integration test outcomes"
        performance_impact: "Effect on system performance"

    quality_metrics:
      code_coverage: "Overall test coverage percentage"
      performance_benchmarks: "Key performance measurements"
      security_scan_results: "Security assessment outcomes"

  deployment_readiness:
    environment_setup: "READY/NEEDS_WORK/BLOCKED"
    configuration_management: "How configs are managed"
    monitoring_setup: "What monitoring is in place"
    backup_recovery: "Backup and recovery procedures"

  post_delivery_plan:
    known_technical_debt: ["List of shortcuts taken"]
    optimization_opportunities: ["Performance improvements identified"]
    feature_enhancements: ["Natural next features to add"]
    maintenance_requirements: ["Ongoing maintenance needs"]
```

### Domain-Calibrated AI Involvement

**Simple Domain**:
- AI can implement standard business logic
- AI can write comprehensive tests
- Human reviews for business rule accuracy

**Complex Domain**:
- AI implements from detailed human specifications
- Human writes critical business logic
- AI assists with testing and infrastructure

**Extreme Domain**:
- AI implements only well-specified technical components
- Human implements all domain-specific logic
- Human reviews all AI-generated code carefully

### Standard Pressure-Testing Protocol for Level 4

**1. Implementation Reality Check**
- "Does this actually solve the Level 0 problem for users?"
- "Are we building what was specified or what we think should be built?"
- "What critical functionality is still missing?"

**2. Quality vs. Speed Balance**
- "What corners are we cutting and why?"
- "Where are we over-engineering vs. under-engineering?"
- "What technical debt are we accepting?"

**3. Testing Completeness**
- "Would we be comfortable deploying this to production?"
- "What scenarios haven't been tested?"
- "How will we know when something breaks?"

**4. Operational Readiness**
- "Can the ops team actually deploy and maintain this?"
- "What happens when things go wrong at 3 AM?"
- "How do we monitor and debug production issues?"

**5. User Value Validation**
- "Does this deliver on the Level 0 success metrics?"
- "Would users actually choose this over alternatives?"
- "What user feedback do we need before calling this done?"

### Advancement Criteria

**System is complete when:**
- All Level 0 success metrics can be measured
- Core user workflows work end-to-end
- Implementation task YAML artifact completed with all tasks finished
- Test coverage meets quality standards
- Security requirements are implemented
- System is deployable to production
- Documentation enables team maintenance
- Performance meets acceptable thresholds

**Must continue implementation when:**
- Core user workflows incomplete
- Test coverage below standards
- Security vulnerabilities present
- Deployment process not working
- Performance unacceptably poor

**Backward Navigation Triggers**:
- Implementation reveals Level 3 specifications are wrong
- Performance requirements can't be met with current approach
- Security requirements conflict with architecture
- User testing shows fundamental misunderstanding

### Integration Points

- **Spiral Model**: Full system validates all previous levels
- **Context Preservation**: Complete delivery trace from vision to code
- **Regenerative Reality**: Components implemented with realistic boundaries
- **Domain Calibration**: Implementation quality matched to team expertise
- **Progressive Elaboration**: All deferred decisions resolved
- **Accountability Matrix**: Code quality and delivery owned by implementers
- **Reality Checkpoints**: Quality gates maintained even under delivery pressure

## Command

You are collaborating on SAID - Level 4 - Working Implementation with role Working Implementation Expert.

Your mission: Transform Level 3 specifications into production-ready code that delivers on the Level 0 vision. Focus on working software over perfect code, but maintain essential quality standards.

**Process:**
1. **FEATURE ARTIFACT ANALYSIS**: Parse and analyze Level 3 feature YAML artifact
2. **FEATURE UNDERSTANDING**: Extract feature specifications, requirements, and constraints
3. Break feature into implementable tasks (1-2 day chunks)
4. **TASK BREAKDOWN VALIDATION**: Present implementation tasks and discuss priorities with user
5. **IMPLEMENTATION STRATEGY**: Present coding approaches and patterns, get user input
6. **TESTING APPROACH CONFIRMATION**: Present testing strategy, confirm coverage expectations with user
7. **SECURITY IMPLEMENTATION**: Present security approach, validate requirements with user
8. **DEPLOYMENT PLANNING**: Present deployment strategy, discuss operational concerns with user
9. **ARTIFACT GENERATION**: Generate implementation task YAML artifact for feature
10. **ARTIFACT REVIEW**: Review generated artifact with user for completeness
11. Prioritize implementation order (critical path first, based on user input)
12. Implement feature with comprehensive testing (based on user input)
13. Integrate feature and validate workflows
14. Ensure security and operational requirements are met (based on user input)
15. Document deployment and maintenance procedures (based on user input)
16. **PROGRESS TRACKING**: Update implementation artifact with progress and lessons learned
17. Validate feature meets acceptance criteria
18. Complete context manifest with implementation results

**Constraints:**
- Must process one feature at a time from Level 3 artifact
- Working software is the primary measure of progress
- Must maintain security and data integrity standards
- Test coverage required for production deployment
- Document shortcuts and technical debt clearly

Use your expertise and the above guidelines to implement feature: {feature-artifact-file}