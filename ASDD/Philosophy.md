# Project Philosophy

_A foundation forged through pressure, not just intention._

> **Core Thesis**: Philosophy emerges when process breaks down. These principles exist to guide judgment when the clean guidelines hit messy reality.

---

## üéØ Philosophical Foundation

### The Collaboration Paradox

AI can offer answers that look complete. **That's the structural trap.** This philosophy applies pressure to both human and AI judgment‚Äînot to break collaboration, but to see how it bends under real project stress.

**Principle**: _Trust through verification, not verification through trust._

- When Claude Code suggests a solution, the first question isn't "Is this right?" but "What assumptions is this making?"
- When guidelines conflict with reality, the reality wins‚Äîbut gets documented as a philosophical update

### Ownership Architecture

**Human Judgment**: Strategy, trade-offs, architectural decisions, failure mode assessment **AI Execution**: Implementation, pattern recognition, systematic application, option generation **Shared Territory**: Code review, testing strategy, documentation structure

**Failure Mode**: When AI starts making architectural decisions or humans start micromanaging syntax. Both indicate boundary drift.

### üèóÔ∏è Regenerative Architecture Principle

**Core Thesis**: Design components that can be easily regenerated by any competent developer (human or AI) with access to clear requirements.

**What "Easily Regenerated" Means:**

- Clear, minimal interfaces that hide implementation complexity
- Self-contained functionality with explicit dependencies
- Well-defined boundaries that resist scope creep
- Comprehensive tests that serve as behavioral specification
- Documentation that explains _why_ decisions were made, not just _what_ was implemented

**The Test**: If this component disappeared tomorrow, could someone rebuild it from the surrounding context and tests without archaeological investigation?

**Why This Matters for AI Collaboration:**

- AI excels at regenerating well-bounded components with clear specs
- AI struggles with archaeological investigation of tightly coupled legacy code
- Clear interfaces make AI-human handoffs much cleaner
- Regenerable components can be safely delegated to AI implementation

---

## ü§ù AI Collaboration Framework

### Core Collaboration Principles

1. **Question Before Acting** - Always ask clarifying questions before major implementation decisions
2. **Present Options** - Provide pros/cons rather than choosing for the user
3. **Explain Reasoning** - Share the "why" behind technical recommendations
4. **Pause for Input** - Honor collaboration checkpoints at key decision points

### Collaboration Failure Modes & Responses

**When AI Assumes Instead of Asks:**

- **Trigger**: Solution appears without options or explanation
- **Response**: "What other approaches did you consider? What would break if we chose differently?"
- **Prevention**: Explicitly request multiple approaches for any non-trivial decision

**When Questions Become Procrastination:**

- **Trigger**: 3+ rounds of questions without progress toward implementation
- **Response**: "Choose the simplest viable approach and we'll iterate."
- **Prevention**: Set decision deadlines: "We need to move on this by end of discussion."

**When Collaboration Fa√ßade Masks Poor Judgment:**

- **Trigger**: AI asks good questions but suggestions reveal misunderstanding
- **Response**: Step back to first principles. "Let's align on what problem we're actually solving."
- **Prevention**: Test understanding with hypotheticals before implementation

### Context-Specific Collaboration Patterns

**Issue Analysis**: "What's the priority: quick fix or robust solution?" **Implementation**: "I can implement this in [X] ways. Which approach resonates with you?" **Testing**: "What testing areas are most important to you for this change?" **Code Review**: "I've identified several improvement opportunities. Which should I prioritize?"

---

## üíª Code Philosophy

### Technical Principles

- **Regenerative architecture first** - Design components that can be easily rebuilt from clear requirements
- **Clarity over cleverness** - Write code that future you will understand
- **Consistency over personal preference** - Follow established patterns
- **Type safety first** - Leverage TypeScript's type system fully
- **Performance awareness** - Consider performance implications, especially for large collections
- **Accessibility by default** - Every UI component should be accessible

### Regenerative Design Application

**Interface Design:**

- Minimize surface area‚Äîexpose only what consumers actually need
- Make dependencies explicit‚Äîno hidden global state or implicit contracts
- Design for testability‚Äîcomplex logic should be easily isolatable

**Component Boundaries:**

- Each component should solve exactly one problem well
- Avoid components that know too much about their context
- Prefer composition over inheritance for flexibility

**Documentation Requirements:**

- Decision logs for non-obvious choices
- Behavioral specifications through tests
- Clear examples of intended usage patterns

### Code Under Pressure

**When principles conflict:**

1. **Safety wins** - Type safety and accessibility are non-negotiable
2. **Consistency breaks ties** - Follow existing patterns unless they're provably wrong
3. **Clarity resolves edge cases** - If future-you wouldn't understand it, current-you needs to document it

**When timeline pressure mounts:**

- Clarity and safety remain non-negotiable
- Performance optimization becomes negotiable (but gets tracked as technical debt)
- Perfect consistency yields to "good enough and documented"

---

## üß™ Testing Strategy

### Test Categories and Separation

**Unit Tests:**

- Test individual functions, classes, or components in isolation
- Use mocked dependencies (APIs, external services, other modules)
- Fast execution (< 1 second per test)
- No network calls, file system access, or browser APIs
- Follow framework-specific naming patterns

**Integration Tests:**

- Test interaction between multiple components or systems
- May use real APIs (with rate limiting considerations)
- Test actual service integration
- Slower execution (may take several seconds)
- Use differentiated naming (*.integration.test.ts)

### Error-Driven Test Development

**The Rule**: When a bug is reported, it MUST get a unit test before fixing

**The Reality**: This rule gets pressured by:

- "It's just a one-line fix"
- "We need this deployed immediately"
- "The test would be harder to write than the fix"

**The Response**: These pressures reveal where your testing strategy needs strengthening. If writing the test is harder than the fix, your code architecture has a problem. If deployment pressure overrides testing discipline, your deployment process has a problem.

**Escape Valve**: Critical production issues can bypass the rule, but:

1. The bypass gets documented with reasoning
2. The test gets written within 48 hours
3. The pattern that enabled the bypass gets addressed

---

## üìã Documentation Strategy

### Documentation serves two masters: Human comprehension and AI context.

**For Humans:**

- Document APIs using language-specific standards (JSDoc, etc.)
- All code that cannot immediately identify why it exists requires documentation
- Focus on _why_ decisions were made, not just _what_ they do

**For AI Collaboration:**

- Maintain decision logs: what approaches were considered and why others were rejected
- Document assumptions explicitly: what must remain true for this code to work
- Include failure mode notes: what breaks first when this approach hits its limits

### Documentation Under Pressure

**When deadlines loom:**

- API documentation remains mandatory
- Decision logs can be simplified but not skipped
- Inline comments for complex logic are non-negotiable

**When AI suggests over-documentation:**

- Test: "Would I reference this in 6 months, or is this noise?"
- Prefer clear code over explanatory comments
- Document the _system_, not the syntax

---

## üîÑ AI Spec-Driven Development (ASDD) - Pressure-Tested

### The Theory

Iterative collaboration with AI to break large specs into implementable tasks through structured phases.

### The Reality Check

**This workflow assumes:**

- Specs can be cleanly decomposed
- AI maintains context across iterations
- Feedback loops improve rather than dilute specifications
- Human judgment can detect when AI has drifted from requirements

**These assumptions fracture when:**

- Domain complexity exceeds AI's pattern matching
- Requirements change faster than the decomposition process
- AI generates structurally plausible but functionally wrong outputs
- The human doesn't understand the domain well enough to guide decomposition

### Workflow with Failure Mode Responses

#### Phase 1: High Level Review

**Input**: High Level Design **Output**: Core Feature Designs, Architecture Design

**Failure Mode**: AI treats all features as equally complex **Response**: Human provides complexity weighting before decomposition

#### Phase 2: Feature Review

**Input**: Core Feature Designs, Architecture Design **Output**: Detailed Designs

**Failure Mode**: Details don't align with architectural constraints **Response**: Architectural review gate‚Äîno progression without alignment verification

#### Phase 3: Detailed Review

**Input**: Detailed Feature Designs, Architecture Designs **Output**: Component Designs

**Failure Mode**: Components assume perfect integration **Response**: Integration complexity assessment for each component boundary

**Regenerative Architecture Check**:

- Can each component be rebuilt from its interface specification?
- What implicit knowledge are we assuming that should be explicit?
- Where are we creating unnecessary coupling?

#### Phase 4: Component Review

<<<<<<< HEAD
**Input**: Detailed Component Designs, Architecture Designs
=======
**Input**: Detailed Component Designs, Architecture Designs
>>>>>>> efca71e (Initial details after some review with AI, before Claude Code.)
**Output**: Issue Designs

**Failure Mode**: Issues don't map to realistic implementation units **Response**: Feasibility check‚Äîcan a single developer implement this in 1-2 days?

**Regenerative Architecture Check**:

- If this component disappeared, what context would someone need to rebuild it?
- What's the minimal interface that makes this component replaceable?
- Are we designing for easy delegation to AI implementation?

#### Phase 5: Issue Review

**Input**: Issue Designs **Output**: Task Lists

**Failure Mode**: Tasks don't account for environmental complexity (build systems, deployment, etc.) **Response**: Infrastructure requirement check for each task

#### Phase 6: Implementation

**Input**: Task Lists **Output**: Implementation

**Failure Mode**: Implementation reveals earlier phases were fundamentally wrong **Response**: Structured rollback‚Äîwhich phase needs revision and why?

### Workflow Escape Valves

**When ASDD becomes counterproductive:**

- Skip to direct implementation for well-understood patterns
- Use lightweight decomposition for experimental work
- Abandon process for time-critical fixes

**When AI context degrades:**

- Reset with explicit context summary
- Switch to human-driven decomposition with AI as implementation partner
- Document where the process broke for future improvement

---

## üõ°Ô∏è Structural Reflexes

### Drift Detection

**Code Quality Drift**:

- Trigger: Test coverage drops, complexity metrics rise, documentation gaps appear
- Response: Immediate architectural review, technical debt assessment

**Collaboration Drift**:

- Trigger: AI starts making decisions without options, human starts micromanaging syntax
- Response: Reset boundaries, clarify decision-making authority

**Process Drift**:

- Trigger: Guidelines get bypassed repeatedly "just this once"
- Response: Update guidelines to match reality or strengthen enforcement

### Philosophy Evolution

This document must change when reality proves it wrong. Changes require:

1. Documentation of what broke and why
2. Assessment of whether the breakage was environmental or structural
3. Updated principles that would have prevented the failure
4. Testing of new principles against historical project pressure

**Anti-Pattern**: Treating this philosophy as immutable doctrine rather than living guidance.

---

## üß≠ Dual Audience Integration

### For Future Developers

This philosophy provides:

- Decision frameworks when guidelines conflict
- Failure mode recognition and response patterns
- Collaboration boundaries with AI tools
- Quality gates that adapt to project pressure

### For Claude Code

This philosophy provides:

- Structured collaboration patterns with clear human judgment checkpoints
- Context for when to question vs. when to execute
- Failure mode recognition for self-correction
- Integration points with existing human decision-making frameworks

**Integration Test**: Can both audiences use this philosophy to make better decisions under pressure? If not, it's documentation theater, not operational philosophy.

---

## üî• Philosophy Under Fire

**When everything breaks:** Revert to core principles‚Äîsafety, clarity, ownership boundaries **When timelines collapse:** Document what gets compromised and why **When AI fails:** Have human-driven fallback patterns ready **When humans fail:** Have structured AI-driven validation ready

This philosophy exists for the moments when clean process hits messy reality. If it only works when everything goes according to plan, it's not philosophy‚Äîit's wishful thinking.

---

_Philosophy version: 0.1_ _Last pressure-tested: 2025-07-02_ _Next philosophical review: When first major failure occurs_
